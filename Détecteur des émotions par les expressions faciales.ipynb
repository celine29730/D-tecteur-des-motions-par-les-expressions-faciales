{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Détecteur des émotions par les expressions faciales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un détecteur des émotions permet de détecter et d’analyser les émotions capturées à un instant t à partir d’une \n",
    "simple photo où une vidéo. Ce service peut identifier jusqu’à 7 émotions : la colère, le dégoût, la peur, le bonheur, neutre, la tristesse et la surprise.\n",
    "Vous allez construire un modèle IA qui permet de réaliser cette tâche sur des images et des vidéos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexte du projet\n",
    "\n",
    "Les expressions du visage peuvent naturellement servirent à évaluer la satisfaction d’un client aux prises avec un service après-vente ou à face à un produit récemment acquis dont il s’agit de comprendre le fonctionnement. On peut encore mentionner les applications suivantes :\n",
    "\n",
    "    La détection d’un manque d’attention chez un conducteur en vue d’augmenter la sécurité de la conduite.\n",
    "    L’évaluation du niveau de stress de passagers à l’atterrissage ou à l’arrivé en gare ou la détection de comportements suspects.\n",
    "    L’humanisation des robots dans leurs interactions avec les humains dont ils prendraient en compte l’état psychique.\n",
    "\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Les expressions faciales humaines peuvent être facilement classées en 7 émotions de base: heureuse, triste, surprise, peur, colère, dégoût et neutre.\n",
    "    Nos émotions faciales sont exprimées par l'activation d'ensembles spécifiques de muscles faciaux. \n",
    "    Ces signaux parfois subtils, mais complexes, dans une expression contiennent souvent une quantité abondante \n",
    "    d'informations sur notre état d'esprit. Grâce à la reconnaissance des émotions faciales, nous sommes en mesure \n",
    "    de mesurer les effets du contenu et des services sur le public / les utilisateurs grâce à une procédure simple \n",
    "    et peu coûteuse. Par exemple, les détaillants peuvent utiliser ces mesures pour évaluer l'intérêt des clients. \n",
    "    Les prestataires de soins de santé peuvent fournir un meilleur service en utilisant des informations supplémentaires \n",
    "    sur l'état émotionnel des patients pendant le traitement. Les producteurs de divertissement peuvent surveiller \n",
    "    l'engagement du public dans les événements pour créer de manière cohérente le contenu souhaité.\n",
    "    \n",
    "    \n",
    "Les émojis ou avatars sont des moyens d'indiquer des signaux non verbaux. Ces indices sont devenus une partie \n",
    "essentielle du chat en ligne, de l'examen des produits, de l'émotion de la marque et bien d'autres. \n",
    "Cela a également conduit à une augmentation de la recherche en science des données dédiée à la narration basée \n",
    "sur les emoji.\n",
    "\n",
    "Grâce aux progrès de la vision par ordinateur et de l'apprentissage en profondeur, il est désormais possible de \n",
    "détecter les émotions humaines à partir d'images. Dans ce projet d'apprentissage en profondeur, nous classerons \n",
    "les expressions faciales humaines pour filtrer et mapper les emojis ou avatars correspondants.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble de données reconnaissance des expressions faciales) se compose d'images de visage en niveaux de gris \n",
    "de 48 * 48 pixels. Les images sont centrées et occupent un espace égal. Cet ensemble de données comprend les \n",
    "émotions faciales des catégories suivantes:\n",
    "\n",
    "    0: colère\n",
    "    1: dégoût\n",
    "    2: peur\n",
    "    3: heureux\n",
    "    4: neutre\n",
    "    5: triste\n",
    "    6: surpris\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons créer un modèle d'apprentissage en profondeur pour classer les expressions faciales à partir des \n",
    "images. Ensuite, nous mapperons l'émotion classée à un emoji ou un avatar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconnaissance des émotions faciales à l'aide de CNN\n",
    "\n",
    "Dans les étapes ci-dessous, on va construire une architecture de réseau de neurones à convolution et on va entraîner le\n",
    "modèle sur l'ensemble de données pour la reconnaissance d'émotion à partir d'images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des bibliothèques\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "#initialisation des générateurs train et test\n",
    "\n",
    "train_dir = 'data/train'\n",
    "val_dir = 'data/test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        val_dir,\n",
    "        target_size=(48,48),\n",
    "        batch_size=64,\n",
    "        color_mode=\"grayscale\",\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction de l'architecture du réseau de convolution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le calque d'entrée a des dimensions prédéterminées et fixes, de sorte que l'image doit être prétraitée avant de \n",
    "pouvoir être introduite dans le calque. Le haar-cascade_frontalface_default.xml dans OpenCV contient des filtres \n",
    "pré-entraînés et utilise Adaboost pour trouver et rogner rapidement le visage.\n",
    "Le visage rogné est ensuite converti en niveaux de gris à l'aide de cv2.cvtColor et redimensionné à 48 par 48 pixels\n",
    "avec cv2.resize. Cette étape réduit considérablement les dimensions par rapport au format RVB d'origine avec \n",
    "trois dimensions de couleur (3, 48, 48). Le pipeline garantit que chaque image peut être introduite dans la \n",
    "couche d'entrée sous la forme d'un tableau numpy (1, 48, 48).\n",
    "\n",
    "Le tableau numpy est passé dans la couche Convolution2D où je spécifie le nombre de filtres comme l'un des \n",
    "hyperparamètres. L'ensemble de filtres (aka. Kernel) est unique avec des poids générés aléatoirement. \n",
    "Chaque filtre, (3, 3) champ réceptif, glisse sur l'image d'origine avec des pondérations partagées pour créer \n",
    "une carte des caractéristiques.\n",
    "La convolution génère des cartes d'entités qui représentent la manière dont les valeurs de pixel sont améliorées,\n",
    "par exemple, la détection des contours et des motifs. Une carte des caractéristiques est créée en appliquant le \n",
    "filtre 1 sur toute l'image. D'autres filtres sont appliqués les uns après les autres pour créer un ensemble de \n",
    "cartes d'entités.\n",
    "\n",
    "* Le pooling est une technique de réduction de dimension généralement appliquée après une ou plusieurs couches \n",
    "convolutives. Il s'agit d'une étape importante lors de la construction de CNN car l'ajout de couches convolutives\n",
    "peut grandement affecter le temps de calcul. On utilise une méthode de mise en commun populaire appelée \n",
    "MaxPooling2D qui utilise (2, 2) fenêtres sur la carte des caractéristiques en ne gardant que la valeur maximale \n",
    "des pixels. Les pixels regroupés forment une image dont les dimensions sont réduites de 4.\n",
    "\n",
    "* La couche dense (Dense Layer)  (aka couches entièrement connectées), est inspirée par la façon dont les \n",
    "neurones transmettent les signaux à travers le cerveau. Il prend un grand nombre d'entités d'entrée et \n",
    "transforme des entités à travers des couches connectées avec des poids entraînables.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation et entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "448/448 [==============================] - 499s 1s/step - loss: 1.8308 - accuracy: 0.2391 - val_loss: 1.7282 - val_accuracy: 0.3298\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 301s 671ms/step - loss: 1.6779 - accuracy: 0.3387 - val_loss: 1.5496 - val_accuracy: 0.4102\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 300s 669ms/step - loss: 1.5648 - accuracy: 0.3953 - val_loss: 1.4948 - val_accuracy: 0.4305\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 298s 665ms/step - loss: 1.4888 - accuracy: 0.4231 - val_loss: 1.4175 - val_accuracy: 0.4668\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 310s 691ms/step - loss: 1.4159 - accuracy: 0.4579 - val_loss: 1.3809 - val_accuracy: 0.4731\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 300s 671ms/step - loss: 1.3682 - accuracy: 0.4798 - val_loss: 1.3315 - val_accuracy: 0.4948\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 303s 676ms/step - loss: 1.3344 - accuracy: 0.4920 - val_loss: 1.2913 - val_accuracy: 0.5149\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 306s 682ms/step - loss: 1.2818 - accuracy: 0.5163 - val_loss: 1.2646 - val_accuracy: 0.5194\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 315s 703ms/step - loss: 1.2510 - accuracy: 0.5287 - val_loss: 1.2344 - val_accuracy: 0.5308\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 305s 681ms/step - loss: 1.2088 - accuracy: 0.5440 - val_loss: 1.2215 - val_accuracy: 0.5329\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 301s 672ms/step - loss: 1.1665 - accuracy: 0.5599 - val_loss: 1.2013 - val_accuracy: 0.5432\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 287s 641ms/step - loss: 1.1532 - accuracy: 0.5689 - val_loss: 1.1734 - val_accuracy: 0.5550\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 284s 634ms/step - loss: 1.1227 - accuracy: 0.5791 - val_loss: 1.1502 - val_accuracy: 0.5678\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 288s 643ms/step - loss: 1.0960 - accuracy: 0.5916 - val_loss: 1.1483 - val_accuracy: 0.5671\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 281s 628ms/step - loss: 1.0662 - accuracy: 0.6010 - val_loss: 1.1353 - val_accuracy: 0.5710\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 287s 641ms/step - loss: 1.0429 - accuracy: 0.6141 - val_loss: 1.1189 - val_accuracy: 0.5790\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 290s 648ms/step - loss: 1.0101 - accuracy: 0.6234 - val_loss: 1.1175 - val_accuracy: 0.5795\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 339s 757ms/step - loss: 0.9915 - accuracy: 0.6300 - val_loss: 1.1169 - val_accuracy: 0.5795\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 305s 680ms/step - loss: 0.9746 - accuracy: 0.6390 - val_loss: 1.1047 - val_accuracy: 0.5845\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 309s 690ms/step - loss: 0.9322 - accuracy: 0.6597 - val_loss: 1.0959 - val_accuracy: 0.5921\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 288s 643ms/step - loss: 0.9179 - accuracy: 0.6618 - val_loss: 1.0899 - val_accuracy: 0.5924\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 288s 643ms/step - loss: 0.8965 - accuracy: 0.6701 - val_loss: 1.0912 - val_accuracy: 0.5965\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 292s 653ms/step - loss: 0.8732 - accuracy: 0.6802 - val_loss: 1.0851 - val_accuracy: 0.6038\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 289s 645ms/step - loss: 0.8422 - accuracy: 0.6887 - val_loss: 1.0834 - val_accuracy: 0.6007\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 307s 685ms/step - loss: 0.8164 - accuracy: 0.7015 - val_loss: 1.0907 - val_accuracy: 0.6037\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 290s 647ms/step - loss: 0.8016 - accuracy: 0.7085 - val_loss: 1.0870 - val_accuracy: 0.6032\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 319s 713ms/step - loss: 0.7717 - accuracy: 0.7168 - val_loss: 1.0792 - val_accuracy: 0.6133\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 320s 714ms/step - loss: 0.7603 - accuracy: 0.7231 - val_loss: 1.0951 - val_accuracy: 0.6083\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 313s 698ms/step - loss: 0.7328 - accuracy: 0.7334 - val_loss: 1.0872 - val_accuracy: 0.6122\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 314s 700ms/step - loss: 0.7050 - accuracy: 0.7425 - val_loss: 1.0967 - val_accuracy: 0.6131\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 304s 678ms/step - loss: 0.6857 - accuracy: 0.7508 - val_loss: 1.0929 - val_accuracy: 0.6164\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 300s 670ms/step - loss: 0.6578 - accuracy: 0.7571 - val_loss: 1.0955 - val_accuracy: 0.6131\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 286s 639ms/step - loss: 0.6383 - accuracy: 0.7700 - val_loss: 1.1061 - val_accuracy: 0.6127\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 306s 682ms/step - loss: 0.6150 - accuracy: 0.7763 - val_loss: 1.1021 - val_accuracy: 0.6124\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 321s 717ms/step - loss: 0.6030 - accuracy: 0.7816 - val_loss: 1.1139 - val_accuracy: 0.6173\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 2796s 6s/step - loss: 0.5814 - accuracy: 0.7913 - val_loss: 1.1261 - val_accuracy: 0.6186\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 285s 635ms/step - loss: 0.5570 - accuracy: 0.8001 - val_loss: 1.1255 - val_accuracy: 0.6165\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 276s 616ms/step - loss: 0.5489 - accuracy: 0.8019 - val_loss: 1.1302 - val_accuracy: 0.6198\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 276s 615ms/step - loss: 0.5146 - accuracy: 0.8137 - val_loss: 1.1329 - val_accuracy: 0.6134\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 277s 618ms/step - loss: 0.5139 - accuracy: 0.8144 - val_loss: 1.1406 - val_accuracy: 0.6152\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 277s 618ms/step - loss: 0.4937 - accuracy: 0.8232 - val_loss: 1.1582 - val_accuracy: 0.6191\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 277s 618ms/step - loss: 0.4673 - accuracy: 0.8336 - val_loss: 1.1552 - val_accuracy: 0.6145\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 276s 616ms/step - loss: 0.4571 - accuracy: 0.8390 - val_loss: 1.1506 - val_accuracy: 0.6211\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 276s 617ms/step - loss: 0.4318 - accuracy: 0.8456 - val_loss: 1.1596 - val_accuracy: 0.6251\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 277s 617ms/step - loss: 0.4242 - accuracy: 0.8486 - val_loss: 1.1961 - val_accuracy: 0.6159\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 278s 620ms/step - loss: 0.4183 - accuracy: 0.8498 - val_loss: 1.2039 - val_accuracy: 0.6217\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 279s 624ms/step - loss: 0.3919 - accuracy: 0.8580 - val_loss: 1.2076 - val_accuracy: 0.6219\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 279s 622ms/step - loss: 0.3731 - accuracy: 0.8638 - val_loss: 1.2144 - val_accuracy: 0.6217\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 280s 624ms/step - loss: 0.3704 - accuracy: 0.8692 - val_loss: 1.2340 - val_accuracy: 0.6229\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 278s 621ms/step - loss: 0.3622 - accuracy: 0.8700 - val_loss: 1.2433 - val_accuracy: 0.6219\n"
     ]
    }
   ],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient une accuracy de notre modèle de 87%, ce qui est un résultat plutôt bon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sauvegarde du modèle pour être utilisé dans l'interface graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En utilisant openCV haarcascade xml, on définit les boîtes englobantes du visage dans la webcam et on prédit les émotions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "emotion_dict = {0: \"Colère\", 1: \"Dégoût\", 2: \"Peur\", 3: \"Heureux\", 4: \"Neutre\", 5: \"Triste\", 6: \"Surpris\"}\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2gray)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1200,860),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autre version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empêche l'utilisation d'openCL et les messages de journalisation inutiles\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "# dictionnaire qui attribue à chaque étiquette une émotion (ordre alphabétique)\n",
    "\n",
    "emotion_dict = {0: \"Colère\", 1: \"Dégoût\", 2: \"Peur\", 3: \"Heureux\", 4: \"Neutre\", 5: \"Triste\", 6: \"Surpris\"}\n",
    "\n",
    "# démarrer le flux webcam\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "# Find haar cascade to draw bounding box around face\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    facecasc = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = facecasc.detectMultiScale(gray,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Video', cv2.resize(frame,(1600,960),interpolation = cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet d'apprentissage en profondeur, nous avons construit un réseau neuronal à convolution pour \n",
    "reconnaître les émotions faciales.On a formé notre modèle et on a  cartographié ces émotions avec les emojis.\n",
    "\n",
    "En utilisant le fichier XML en cascade de haar d'OpenCV, nous obtenons la boîte englobante des visages dans la \n",
    "webcam. Ensuite, nous transmettons ces boîtes au modèle entraîné pour la classification.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
